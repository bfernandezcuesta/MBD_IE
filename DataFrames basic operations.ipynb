{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames Hands-on: my first exercise with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Flights dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to a 2010 report made by the US Federal Aviation Administration, the economic price of domestic flight delays entails a yearly cost of 32.9 billion dollars to passengers, airlines and other parts of the economy. More than half of that amount comes from passengers' pockets, as they do not only waste time waiting for their planes to leave, but also miss connecting flights, spend money on food and have to sleep on hotel rooms while they're stranded.\n",
    "\n",
    "The report, focusing on data from year 2007, estimated that air transportation delays put a 4 billion dollar dent in the country's gross domestic product that year. Full report can be found \n",
    "<a href=\"http://www.isr.umd.edu/NEXTOR/pubs/TDI_Report_Final_10_18_10_V3.pdf\">here</a>.\n",
    "\n",
    "But which are the causes for these delays?\n",
    "\n",
    "In order to answer this question, we are going to analyze the provided dataset, containing up to 1.936.758 different internal flights in the US for 2008 and their causes for delay, diversion and cancellation; if any.\n",
    "\n",
    "The data comes from the U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS)\n",
    "\n",
    "This dataset is composed by the following variables:\n",
    "1. **Year** 2008\n",
    "2. **Month** 1\n",
    "3. **DayofMonth** 1-31\n",
    "4. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "5. **DepTime** actual departure time (local, hhmm)\n",
    "6. **CRSDepTime** scheduled departure time (local, hhmm)\n",
    "7. **ArrTime** actual arrival time (local, hhmm)\n",
    "8. **CRSArrTime** scheduled arrival time (local, hhmm)\n",
    "9. **UniqueCarrie**r unique carrier code\n",
    "10. **FlightNum** flight number\n",
    "11. **TailNum** plane tail number: aircraft registration, unique aircraft identifier\n",
    "12. **ActualElapsedTime** in minutes\n",
    "13. **CRSElapsedTime** in minutes\n",
    "14. **AirTime** in minutes\n",
    "15. **ArrDelay** arrival delay, in minutes: A flight is counted as \"on time\" if it operated less than 15 minutes later the scheduled time shown in the carriers' Computerized Reservations Systems (CRS).\n",
    "16. **DepDelay** departure delay, in minutes\n",
    "17. **Origin** origin IATA airport code\n",
    "18. **Dest** destination IATA airport code\n",
    "19. **Distance** in miles\n",
    "20. **TaxiIn** taxi in time, in minutes\n",
    "21. **TaxiOut** taxi out time in minutes\n",
    "22. **Cancelled** *was the flight cancelled\n",
    "23. **CancellationCode** reason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n",
    "24. **Diverted** 1 = yes, 0 = no\n",
    "25. **CarrierDelay** in minutes: Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.\n",
    "26. **WeatherDelay** in minutes: Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.\n",
    "27. **NASDelay** in minutes: Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc.\n",
    "28. **SecurityDelay** in minutes: Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.\n",
    "29. **LateAircraftDelay** in minutes: Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV file using Spark's default delimiter (\",\"). The first line contains the headers so it is not part of the data. Hence we set the header option to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does nothing: Spark is lazy so the read operation will be deferred until an action is executed\n",
    "flightsDF = spark.read.option(\"header\", \"true\").csv(\"flights_jan08.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the schema (column types) of the data. This is just metadata, so it is not an action\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is a string because we did not specify the data type for each column, neither asked Spark to try to infer the schema of the data. We do not want all columns to be strings because there are a few numeric ones that should be treated as such. Let's request Spark to infer the schema, based on the data types found. Keep in mind this is slower than directly providing a schema for our data, which is the recommended option if we know our data and are sure of the data type each column should have. We will demonstrate this later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF = spark.read\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .option(\"inferSchema\", \"true\")\\\n",
    "                 .csv(\"flights_jan08.csv\")\n",
    "\n",
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things look better now. There are a few integer columns involving the year, the month, the day of month and day of the week, as well as distances, flight number and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run an action on this, such as printing the first few rows to execute the DAG until now (which has just one step - the read)\n",
    "flightsDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the very first things we want to know about our data is how big they are in terms of rows and columns. How many examples we have and how many variables we are dealing with. Since we will be performing multiple operations with our flightsDF DataFrame, let's `cache`() it so that Spark keeps it in memory instead of just freeing the memory after each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column names. This is just metadata\n",
    "print(\"Our data have\", len(flightsDF.columns), \"columns\")\n",
    "\n",
    "flightsDF.cache()        # This does nothing, but Spark takes note to keep the DF in memory after the first time it is materialized\n",
    "rows = flightsDF.count() # This is an action and therefore, flightsDF will be materialized\n",
    "print(\"Our data have\", rows, \"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 100,000 rows and 29 variables. Not bad! Let's start looking at some simple transformations we can do with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.select(\"Year\", \"Month\", \"DayofMonth\", \"ArrTime\", \"FlightNum\")\\\n",
    "         .show() # column names are case-sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter (retain) rows according to the values of one or more columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the DataFrame transformations are defined in package `pyspark.sql.functions`, it is common to import the entire package with an alias like `F`, instead of importing each individual function. Then, we use `F.` before each function to tell\n",
    "python where to look for that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F  \n",
    "\n",
    "# function col is used to indicate that we are referring to a column whose name is the argument\n",
    "\n",
    "flightsJanuary20 = flightsDF\\\n",
    "                      .where(F.col(\"DayofMonth\") == 20)\\\n",
    "                      .select(\"Year\", \"Month\", \"ArrTime\") # this does not launch anything because Spark is lazy\n",
    "\n",
    "# Let's see how many flights we have on January 20, 2008\n",
    "# count() is an action so it launches the computations. If we hadn't cached the data, it would read the CSV again.\n",
    "rowsJanuary20 = flightsJanuary20.count()\n",
    "print(\"We have\", rowsJanuary20, \"flights that traveled on January 20, 2008\")\n",
    "\n",
    "# This is another action on the flightsJanuary DataFrame. It is NOT cached, \n",
    "# hence the \"where\" and \"select\" operations are launched again.\n",
    "flightsJanuary20.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to cache your DataFrame if you will be doing multiple operations on it, or you will be re-doing \n",
    "the same computations many times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter() is an alias for where() and they both do exactly the same:\n",
    "flightsDF.filter(F.col(\"DayofMonth\") == 20)\n",
    "\n",
    "# It is interesting to note that you can indicate the filter condition just as a SQL fragment:\n",
    "flightsJanuary31 = flightsDF.where(\"DayofMonth = 31\")\n",
    "flightsJanuary31.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>REMEMBER:</b> Spark is doing these operations in a distributed fashion in the cluster, so each node of the cluster is filtering rows locally among those present in that node and counts the rows retained in that node. Every node (more precisely, every <i>executor</i>) sends the count to the driver where the partial results are aggregated to obtain the final count.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>YOUR TURN</b>: show the arrival delay, origin and destination airports of those flights scheduled on Sundays having an \n",
    "arrival delay greater than 15 minutes. Check the schema again for the exact name of those columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a complex condition consisting of two conditions that must be fulfilled simultaneously\n",
    "delayedOnSundaysDF = flightsDF.where((<FILL IN HERE>) &\n",
    "                                     (<FILL IN HERE>))\\\n",
    "                              .select(<FILL IN HERE>)\\\n",
    "                              .show()\n",
    "\n",
    "delayedOnSundaysDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting unique rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested to know how many distinct values a column has, or even how many distinct rows our complete dataset has. If we consider all the columns together, it is unlikely that we have two rows that are exactly the same, but if we select only a few columns, then it is much more likely to have two or more rows having the same value in all the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctFlights = flightsDF.distinct()  # distinct is a transformation returning a new DataFrame without duplicated rows\n",
    "distinctFlightsCount = distinctFlights.count()\n",
    "\n",
    "print(\"There are\", distinctFlightsCount, \"distinct rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, there are no two rows with the exact same values. However, if we select only two columns, say `Origin` and `Dest`, then we get a DataFrame of only two columns which may have repeated rows as there are multiple flights having the same origin and destination, although they differ in other columns like the date, flight number, etc, but those columns are not in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.select(\"Origin\", \"Dest\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>YOUR TURN</b>: let's see how many combinations we have of Origin and Dest, i.e. how many distinct routes there are\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer in the next cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see how many combinations we have of Origin and Dest\n",
    "distinctFlightsOriginDest = flightsDF.<FILL IN HERE>(<FILL IN HERE>).<FILL IN HERE>\n",
    "\n",
    "originDestCombinations = distinctFlightsOriginDest.<FILL IN HERE>\n",
    "\n",
    "print(\"There are\", originDestCombinations, \"combinations of an origin airport and a dest airport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have 100,000 rows, there are only 1009 distinct combinations of an origin an a destination airport.\n",
    "\n",
    "If we select only one column and do the same, we get the number of distinct values of that column. Let's do that for the origin airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctOrigins = flightsDF.<FILL IN HERE>.....\n",
    "print(\"There are\", distinctOrigins, \"airports from which flights may depart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>YOUR TURN</b>: let's do this for every column, in a loop, in order to have an idea of the kind of data we have. Probably this makes sense for some columns (those that are categorical) and does not make sense for numeric ones since every row may have a different value. But let's try and explore the results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columnName in flights.columns:\n",
    "    distinctValues = flights.select(<FILL IN HERE>)\\\n",
    "                            .<FILL IN HERE>()\\\n",
    "                            .<FILL IN HERE>()\n",
    "    \n",
    "    # Don't forget to indent this line to indicate it is also inside the loop\n",
    "    print(\"There are\", distinctValues, \"distinct values in column\", columnName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating or replacing a column by operating with existing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the flight distances in miles. Let's convert them to kilometers. 1 mile equals 1.61 kilometers so we just need to multiply the miles by 1.61. Again, each node will do this operation locally over the rows located in that node. No data movement, no information needed about the data of other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn is a transformation returing a new DataFrame with one extra column appended on the right\n",
    "flightsWithKm = flightsDF.withColumn(\"DistanceKm\", F.col(\"Distance\") * 1.61)\n",
    "\n",
    "flightsWithKm.printSchema()\n",
    "\n",
    "flightsWithKm.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The day of the week is encoded as an integer variable. Using an integer or using a categorical variable is a decision that depends on how the variable will be used when fitting a model. Does it make sense to consider \"greater\" or \"lower\" days, i.e., something that increases or occurs more as the day of the week increases from Monday to Sunday? Just for demonstration, we are going to **replace** column DayOfWeek by a string with the day of the week, from Monday to Sunday. We just call `withColumn` but pass the name of an existing column. In that case we are replacing a column, not  creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flightsCategoricalDay = flightsDF.withColumn(\"DayOfWeek\", F.when(F.col(\"DayOfWeek\") == 1, \"Monday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 2, \"Tuesday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 3, \"Wednesday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 4, \"Thursday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 5, \"Friday\")\\\n",
    "                                                           .when(F.col(\"DayOfWeek\") == 6, \"Saturday\")\\\n",
    "                                                           .otherwise(\"Sunday\"))\n",
    "\n",
    "flightsCategoricalDay.printSchema() # the column is still in the same position but has now string type\n",
    "\n",
    "flightsCategoricalDay.select(\"DayOfWeek\", \"DepTime\", \"ArrTime\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP:</b> The process of creating new variables (called <i>features</i>) from existing ones, or incorporating features from external data sources (sometimes public data), as well as cleaning or replacing features after applying some normalization technique is known as <b>feature engineering</b>. Sometimes it has a lot to do with domain-specific knowledge, but also with some mathematical and statistical tricks concerning feature normalization or well-kown general-purpose transformations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `when` is very common to re-categorize a variable, or to create variables (especially categorical variables) whose values depend on one or multiple conditions on the values of other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>YOUR TURN</b>: let's create a (categorical) string variable with two categories, indicating whether the day of the week was a working day (\"working\") or a weekend day (\"weekend\"). Use `withColumn` and `when` to create a new column with the name of your choice (should be clear and concise). A working day is that whose DayOfWeek is between 1 and 5, both included. Otherwise it is a weekend day.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDayPart = flightsDF.withColumn(\"dayPart\", F.when(<FILL IN HERE>, \"morning\")\\\n",
    "                                                  .when(<FILL IN HERE>)\\\n",
    "                                                  .when(<FILL IN HERE>)\\\n",
    "                                                  .otherwise(\"<FILL IN HERE>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns and selecting them on-the-fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`withColumn` is not the only way to create columns. We can use `select` not only to select existing columns, but to create a new column and select it for the resulting DataFrame, all at the same time. Let's see an example: we are going to select the origin and destination airports, which already exist in our data, and a newly created column with the distance in kilometers. When selecting a new column, it is usual to give it a name using function `alias`. Otherwise, Spark will give it an auto-generated name from the operations employed (an ugly name like \"1.6 * Distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsAirportsAndKm = flightsDF.select(F.col(\"Origin\"),\\\n",
    "                                        F.col(\"Dest\"),\\\n",
    "                                        (1.6 * F.col(\"Distance\")).alias(\"DistanceKm\"))\n",
    "\n",
    "flightsAirportsAndKm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>YOUR TURN</b>: create a new DataFrame of three columns by selecting columns \"FlightNum\", \"TailNum\" and a newly created string column by concatenating the contents of column \"FlightNum\" and \"TailNum\" with a \"-\". Use function <i>concat_ws</i>(concatenate with separator) with the syntax concat_ws(\"-\", column1, column2) from pyspark.sql.functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsNumTail = flightsDF.select(<FILL IN HERE>,\n",
    "                                  <FILL IN HERE>,\n",
    "                                  F.concat_ws(\"-\", <FILL IN HERE>).alias(\"<FILL IN HERE>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting incorrectly parsed columns to the correct data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very important. Most often, Spark will not correctly infer the data type of every column. Something very common is that a column that should be numerical is not recognized as such, due to the data having a label like \"NA\" that was meant to indicate a missing value. However, Spark does not acknowledge the string \"NA\" as representing a missing value so it infers string type for a column whenever it finds a mixture of numbers and strings. For that reason, columns like `ArrDelay` and `DepDelay` have been inferred as String while they are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naCount = flightsDF.where(\"ArrDelay = 'NA'\").count()\n",
    "# The where function could also have been specified as .where(F.col(\"ArrDelay\") == \"NA\"). Both are equivalent.\n",
    "\n",
    "print(\"There are \", naCount, \"rows with NA in ArrDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>YOUR TURN</b>: replace flightsDF by the result of selecting the rows in which ArrDelay is not 'NA' AND DepDelay is not 'NA'. Use a where operation with a condition containing the & of two simple conditions. Then, cast ArrDelay and DepDelay to integer using the code provided. NOTE: operator \"not equals\" is != in python.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "flightsDF = flightsDF.where((<FILL IN HERE>) &\\\n",
    "                            (<FILL IN HERE>))\\\n",
    "                     .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(IntegerType()))\\\n",
    "                     .withColumn(\"DepDelay\", F.col(\"DepDelay\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation functions on the whole DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides distributed implementations of common aggregation measures such as the mean, min, max, and standard deviation among others. All the functions receive a single argument that should be the column where the function must be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the flights with an ArrDelay greater than 15 minutes and from them, let's show the min, max, mean and standard deviation of the `ArrDelay`. We will do it by creating and selecting those columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we select those flights with at least 16 minutes of delay and then compute the aggregations\n",
    "flightsDF.where(F.col(\"ArrDelay\") > 15)\\\n",
    "          .select(F.mean(\"ArrDelay\").alias(\"MeanArrDelay\"),\\\n",
    "                  F.min(\"ArrDelay\").alias(\"MinArrDelay\"),\\\n",
    "                  F.max(\"ArrDelay\").alias(\"MaxArrDelay\"),\n",
    "                  F.stddev(\"ArrDelay\").alias(\"StddevArrDelay\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a Spark function that already does this for us, called `summary`. It does it for every numeric column it finds in the dataset. It is a transformation so it returns a new DataFrame that contains the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summariesDF = flightsDF.summary()\n",
    "summariesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting a Spark DataFrame to a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to understand the results. Hence we are going to convert this Spark DataFrame to a Pandas dataframe which has a pretty output when printing and it is easier to visualize its contents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TIP</b>: Note that the concept of dataframe as a table with rows and columns exists in many programming languages and libraries (Python, R, pyspark). However, Spark deals with physically distributed DataFrames, nothing to do with Python or R which run on a single machine. Converting a Spark DataFrame into a Pandas dataframe entails collecting all rows to the driver, which could potentially result in an Out-of-Memory exception since the complete contents may be larger than the RAM memory of a single machine where the driver process is running. In this case, we are collecting just the summaries, which are very small so there is no risk.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.summary().toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
